{"documents": ["PROMISE12"], "year": 2016, "keyphrase_query": "3d image segmentation mri biomedical", "abstract": "Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods.", "task": "3D image segmentation", "domain": "biomedical", "modality": "MRI", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We propose a CNN trained end-to-end on 3D MRI volumes depicting prostates to predict segmentation for the whole volume at once."}
{"documents": ["COCO Captions"], "year": 2017, "keyphrase_query": "text generation", "abstract": "Automatically generating coherent and semantically meaningful text has many applications in machine translation, dialogue systems, image captioning, etc. Recently, by combining with policy gradient, Generative Adversarial Nets (GAN) that use a discriminative model to guide the training of the generative model as a reinforcement learning policy has shown promising results in text generation. However, the scalar guiding signal is only available after the entire text has been generated and lacks intermediate information about text structure during the generative process. As such, it limits its success when the length of the generated text samples is long (more than 20 words). In this paper, we propose a new framework, called LeakGAN, to address the problem for long text generation. We allow the discriminative net to leak its own high-level extracted features to the generative net to further help the guidance. The generator incorporates such informative signals into all generation steps through an additional Manager module, which takes the extracted features of current generated words and outputs a latent vector to guide the Worker module for next-word generation. Our extensive experiments on synthetic data and various real-world tasks with Turing test demonstrate that LeakGAN is highly effective in long text generation and also improves the performance in short text generation scenarios. More importantly, without any supervision, LeakGAN would be able to implicitly learn sentence structures only through the interaction between Manager and Worker.", "task": "text generation", "domain": "", "modality": "Text", "language": "", "training_style": "Semi-supervised", "text_length": "Paragraph-level", "query": "We propose a framework for using Generative Adversarial Networks for long text generation."}
{"documents": ["BraTS 2015"], "year": 2017, "keyphrase_query": "brain lesion segmentation image biomedical", "abstract": "HIGHLIGHTSAn efficient 11‐layers deep, multi‐scale, 3D CNN architecture.A novel training strategy that significantly boosts performance.The first employment of a 3D fully connected CRF for post‐processing.State‐of‐the‐art performance on three challenging lesion segmentation tasks.New insights into the automatically learned intermediate representations. ABSTRACT We propose a dual pathway, 11‐layers deep, three‐dimensional Convolutional Neural Network for the challenging task of brain lesion segmentation. The devised architecture is the result of an in‐depth analysis of the limitations of current networks proposed for similar applications. To overcome the computational burden of processing 3D medical scans, we have devised an efficient and effective dense training scheme which joins the processing of adjacent image patches into one pass through the network while automatically adapting to the inherent class imbalance present in the data. Further, we analyze the development of deeper, thus more discriminative 3D CNNs. In order to incorporate both local and larger contextual information, we employ a dual pathway architecture that processes the input images at multiple scales simultaneously. For post‐processing of the network's soft segmentation, we use a 3D fully connected Conditional Random Field which effectively removes false positives. Our pipeline is extensively evaluated on three challenging tasks of lesion segmentation in multi‐channel MRI patient data with traumatic brain injuries, brain tumours, and ischemic stroke. We improve on the state‐of‐the‐art for all three applications, with top ranking performance on the public benchmarks BRATS 2015 and ISLES 2015. Our method is computationally efficient, which allows its adoption in a variety of research and clinical settings. The source code of our implementation is made publicly available.", "task": "brain lesion segmentation", "domain": "biomedical", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "I want to train a 3D CNN for brain lesion segmentation"}
{"documents": ["DUC 2004", "JAMUL", "JNC"], "year": 2019, "keyphrase_query": "abstractive summarization text", "abstract": "Neural encoder-decoder models have been successful in natural language generation tasks. However, real applications of abstractive summarization must consider additional constraint that a generated summary should not exceed a desired length. In this paper, we propose a simple but effective extension of a sinusoidal positional encoding (Vaswani et al., 2017) to enable neural encoder-decoder model to preserves the length constraint. Unlike in previous studies where that learn embeddings representing each length, the proposed method can generate a text of any length even if the target length is not present in training data. The experimental results show that the proposed method can not only control the generation length but also improve the ROUGE scores.", "task": "abstractive summarization", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "Paragraph-level", "query": "I want to train an abstractive summarizer wherein the model preserves the length-constraint of the generated summary."}
{"documents": ["PASCAL VOC", "PASCAL VOC 2007"], "year": 2017, "keyphrase_query": "object detection image", "abstract": "Of late, weakly supervised object detection is with great importance in object recognition. Based on deep learning, weakly supervised detectors have achieved many promising results. However, compared with fully supervised detection, it is more challenging to train deep network based detectors in a weakly supervised manner. Here we formulate weakly supervised detection as a Multiple Instance Learning (MIL) problem, where instance classifiers (object detectors) are put into the network as hidden nodes. We propose a novel online instance classifier refinement algorithm to integrate MIL and the instance classifier refinement procedure into a single deep network, and train the network end-to-end with only image-level supervision, i.e., without object location information. More precisely, instance labels inferred from weak supervision are propagated to their spatially overlapped instances to refine instance classifier online. The iterative instance classifier refinement procedure is implemented using multiple streams in deep network, where each stream supervises its latter stream. Weakly supervised object detection experiments are carried out on the challenging PASCAL VOC 2007 and 2012 benchmarks. We obtain 47% mAP on VOC 2007 that significantly outperforms the previous state-of-the-art.", "task": "object detection", "domain": "", "modality": "Image", "language": "", "training_style": "weakly supervised", "text_length": "", "query": "I want to train an object detector in a weakly supervised fashion where I only have access to image-level information and not object-level."}
{"documents": ["IJB-A", "IJB-B", "VGGFace2"], "year": 2018, "keyphrase_query": "face recognition image", "abstract": "The objective of this paper is to learn a compact representation of image sets for template-based face recognition. We make the following contributions: first, we propose a network architecture which aggregates and embeds the face descriptors produced by deep convolutional neural networks into a compact fixed-length representation. This compact representation requires minimal memory storage and enables efficient similarity computation. Second, we propose a novel GhostVLAD layer that includes ghost clusters, that do not contribute to the aggregation. We show that a quality weighting on the input faces emerges automatically such that informative images contribute more than those with low quality, and that the ghost clusters enhance the network’s ability to deal with poor quality images. Third, we explore how input feature dimension, number of clusters and different training techniques affect the recognition performance. Given this analysis, we train a network that far exceeds the state-of-the-art on the IJB-B face recognition dataset. This is currently one of the most challenging public benchmarks, and we surpass the state-of-the-art on both the identification and verification protocols.", "task": "face recognition", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I propose a method to learn representations of images for face recogntion"}
{"documents": ["CoNLL-2012"], "year": 2018, "keyphrase_query": "semantic role labeling text news english", "abstract": "Current state-of-the-art semantic role labeling (SRL) uses a deep neural network with no explicit linguistic features. However, prior work has shown that gold syntax trees can dramatically improve SRL decoding, suggesting the possibility of increased accuracy from explicit modeling of syntax. In this work, we present linguistically-informed self-attention (LISA): a neural network model that combines multi-head self-attention with multi-task learning across dependency parsing, part-of-speech tagging, predicate detection and SRL. Unlike previous models which require significant pre-processing to prepare linguistic features, LISA can incorporate syntax using merely raw tokens as input, encoding the sequence only once to simultaneously perform parsing, predicate detection and role labeling for all predicates. Syntax is incorporated by training one attention head to attend to syntactic parents for each token. Moreover, if a high-quality syntactic parse is already available, it can be beneficially injected at test time without re-training our SRL model. In experiments on CoNLL-2005 SRL, LISA achieves new state-of-the-art performance for a model using predicted predicates and standard word embeddings, attaining 2.5 F1 absolute higher than the previous state-of-the-art on newswire and more than 3.5 F1 on out-of-domain data, nearly 10% reduction in error. On ConLL-2012 English SRL we also show an improvement of more than 2.5 F1. LISA also out-performs the state-of-the-art with contextually-encoded (ELMo) word representations, by nearly 1.0 F1 on news and more than 2.0 F1 on out-of-domain text.", "task": "semantic role labeling", "domain": "news", "modality": "Text", "language": "English", "training_style": "", "text_length": "Sentence-level", "query": "We propose a linguistically-informed approach to semantic role labeling with neural networks that uses multi-task learning"}
{"documents": ["Caltech Pedestrian Dataset", "INRIA Person"], "year": 2014, "keyphrase_query": "object detection image pedestrian", "abstract": "Even with the advent of more sophisticated, data-hungry methods, boosted decision trees remain extraordinarily successful for fast rigid object detection, achieving top accuracy on numerous datasets. While effective, most boosted detectors use decision trees with orthogonal (single feature) splits, and the topology of the resulting decision boundary may not be well matched to the natural topology of the data. Given highly correlated data, decision trees with oblique (multiple feature) splits can be effective. Use of oblique splits, however, comes at considerable computational expense. Inspired by recent work on discriminative decorrelation of HOG features, we instead propose an efficient feature transform that removes correlations in local neighborhoods. The result is an overcomplete but locally decorrelated representation ideally suited for use with orthogonal decision trees. In fact, orthogonal trees with our locally decorrelated features outperform oblique trees trained over the original features at a fraction of the computational cost. The overall improvement in accuracy is dramatic: on the Caltech Pedestrian Dataset, we reduce false positives nearly tenfold over the previous state-of-the-art.", "task": "object detection", "domain": "pedestrian", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We propose an method for pedestrian detection using boosted decision trees"}
{"documents": ["WMT 2014"], "year": 2014, "keyphrase_query": "machine translation english,french", "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.", "task": "machine translation", "domain": "", "modality": "", "language": "English,French", "training_style": "", "text_length": "", "query": "I want to develop a model that uses encoder-decoder neural network models for machine translation"}
{"documents": ["WMT 2014"], "year": 2017, "keyphrase_query": "machine translation text", "abstract": "Depthwise separable convolutions reduce the number of parameters and computation used in convolutional operations while increasing representational efficiency. They have been shown to be successful in image classification models, both in obtaining better models than previously possible for a given parameter count (the Xception architecture) and considerably reducing the number of parameters required to perform at a given level (the MobileNets family of architectures). Recently, convolutional sequence-to-sequence networks have been applied to machine translation tasks with good results. In this work, we study how depthwise separable convolutions can be applied to neural machine translation. We introduce a new architecture inspired by Xception and ByteNet, called SliceNet, which enables a significant reduction of the parameter count and amount of computation needed to obtain results like ByteNet, and, with a similar parameter count, achieves new state-of-the-art results. In addition to showing that depthwise separable convolutions perform well for machine translation, we investigate the architectural changes that they enable: we observe that thanks to depthwise separability, we can increase the length of convolution windows, removing the need for filter dilation. We also introduce a new\"super-separable\"convolution operation that further reduces the number of parameters and computational cost for obtaining state-of-the-art results.", "task": "machine translation", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "", "query": "A new deep neural network architecture for machine translation using depthwise separable convolutions"}
{"documents": ["CoNLL 2002", "CoNLL-2003"], "year": 2016, "keyphrase_query": "named entity recognition text", "abstract": "State-of-the-art named entity recognition systems rely heavily on hand-crafted features and domain-specific knowledge in order to learn effectively from the small, supervised training corpora that are available. In this paper, we introduce two new neural architectures---one based on bidirectional LSTMs and conditional random fields, and the other that constructs and labels segments using a transition-based approach inspired by shift-reduce parsers. Our models rely on two sources of information about words: character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora. Our models obtain state-of-the-art performance in NER in four languages without resorting to any language-specific knowledge or resources such as gazetteers.", "task": "named entity recognition", "domain": "", "modality": "Text", "language": "", "training_style": "supervised and unsupervised", "text_length": "", "query": "Two new neural architectures for named entity recognition which rely on two sources of information about words: character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora"}
{"documents": ["T-LESS"], "year": 2019, "keyphrase_query": "object detection, 6d pose estimation image", "abstract": "We propose a real-time RGB-based pipeline for object detection and 6D pose estimation. Our novel 3D orientation estimation is based on a variant of the Denoising Autoencoder that is trained on simulated views of a 3D model using Domain Randomization. This so-called Augmented Autoencoder has several advantages over existing methods: It does not require real, pose-annotated training data, generalizes to various test sensors and inherently handles object and view symmetries. Instead of learning an explicit mapping from input images to object poses, it provides an implicit representation of object orientations defined by samples in a latent space. Our pipeline achieves state-of-the-art performance on the T-LESS dataset both in the RGB and RGB-D domain. We also evaluate on the LineMOD dataset where we can compete with other synthetically trained approaches. We further increase performance by correcting 3D orientation estimates to account for perspective errors when the object deviates from the image center and show extended results.", "task": "object detection, 6D pose estimation", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "A real-time RGB-based pipeline based on a variant of the Denoising Autoencoder that is trained on simulated views of a 3D model using Domain Randomization"}
{"documents": ["IJB-A", "PRID2011", "iLIDS-VID"], "year": 2017, "keyphrase_query": "set recognition image", "abstract": "This paper targets on the problem of set to set recognition, which learns the metric between two image sets. Images in each set belong to the same identity. Since images in a set can be complementary, they hopefully lead to higher accuracy in practical applications. However, the quality of each sample cannot be guaranteed, and samples with poor quality will hurt the metric. In this paper, the quality aware network (QAN) is proposed to confront this problem, where the quality of each sample can be automatically learned although such information is not explicitly provided in the training stage. The network has two branches, where the first branch extracts appearance feature embedding for each sample and the other branch predicts quality score for each sample. Features and quality scores of all samples in a set are then aggregated to generate the final feature embedding. We show that the two branches can be trained in an end-to-end manner given only the set-level identity annotation. Analysis on gradient spread of this mechanism indicates that the quality learned by the network is beneficial to set-to-set recognition and simplifies the distribution that the network needs to fit. Experiments on both face verification and person re-identification show advantages of the proposed QAN. The source code and network structure can be downloaded at this https URL", "task": "set to set recognition", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "A quality-aware network for set-to-set recognition of images"}
{"documents": ["CIFAR-FS"], "year": 2019, "keyphrase_query": "image classification", "abstract": "Meta-learning has been proposed as a framework to address the challenging few-shot learning setting. The key idea is to leverage a large number of similar few-shot tasks in order to learn how to adapt a base-learner to a new task for which only a few labeled samples are available. As deep neural networks (DNNs) tend to overfit using a few samples only, meta-learning typically uses shallow neural networks (SNNs), thus limiting its effectiveness. In this paper we propose a novel few-shot learning method called meta-transfer learning (MTL) which learns to adapt a deep NN for few shot learning tasks. Specifically, \"meta\" refers to training multiple tasks, and \"transfer\" is achieved by learning scaling and shifting functions of DNN weights for each task. In addition, we introduce the hard task (HT) meta-batch scheme as an effective learning curriculum for MTL. We conduct experiments using (5-class, 1-shot) and (5-class, 5-shot) recognition tasks on two challenging few-shot learning benchmarks: miniImageNet and Fewshot-CIFAR100. Extensive comparisons to related works validate that our meta-transfer learning approach trained with the proposed HT meta-batch scheme achieves top performance. An ablation study also shows that both components contribute to fast convergence and high accuracy.", "task": "image classification", "domain": "", "modality": "Image", "language": "", "training_style": "Few-shot", "text_length": "", "query": "We propose a method for few-shot learning called meta transfer learning."}
{"documents": ["WMT 2015"], "year": 2016, "keyphrase_query": "machine translation text", "abstract": "The existing machine translation systems, whether phrase-based or neural, have relied almost exclusively on word-level modelling with explicit segmentation. In this paper, we ask a fundamental question: can neural machine translation generate a character sequence without any explicit segmentation? To answer this question, we evaluate an attention-based encoder-decoder with a subword-level encoder and a character-level decoder on four language pairs--En-Cs, En-De, En-Ru and En-Fi-- using the parallel corpora from WMT'15. Our experiments show that the models with a character-level decoder outperform the ones with a subword-level decoder on all of the four language pairs. Furthermore, the ensembles of neural models with a character-level decoder outperform the state-of-the-art non-neural machine translation systems on En-Cs, En-De and En-Fi and perform comparably on En-Ru.", "task": "machine translation", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "Sentence-level", "query": "We propose a method for character-level machine translation."}
{"documents": ["WMT 2014"], "year": 2014, "keyphrase_query": "machine translation text", "abstract": "In this paper, we propose a novel neural network model called RNN Encoder‐ Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder‐Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.", "task": "Machine translation", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We propose a novel neural network architecture that encodes source text into a fixed-length vector representation before subsequently decoding it out into target text. "}
{"documents": ["QM9"], "year": 2017, "keyphrase_query": "molecular property prediction molecules", "abstract": "Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.", "task": "molecular property prediction", "domain": "", "modality": "Molecules", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We describe a new supervised learning framework for molecule prediction that is invariant to molecular symmetries."}
{"documents": ["WMT 2014"], "year": 2016, "keyphrase_query": "natural language generation text", "abstract": "We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a \\textit{critic} network that is trained to predict the value of an output token, given the policy of an \\textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling.", "task": "natural language generation", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We use actor-critic methods to learn to generate language by directly optimizing for task-specific scores like BLEU."}
{"documents": ["COCO-QA", "Visual Question Answering"], "year": 2015, "keyphrase_query": "visual question answering image text", "abstract": "We describe a very simple bag-of-words baseline for visual question answering. This baseline concatenates the word features from the question and CNN features from the image to predict the answer. When evaluated on the challenging VQA dataset [2], it shows comparable performance to many recent approaches using recurrent neural networks. To explore the strength and weakness of the trained model, we also provide an interactive web demo and open-source code. .", "task": "visual question answering", "domain": "", "modality": "image and text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We propose a simple baseline for visual question answering."}
{"documents": ["CUB-200-2011"], "year": 2014, "keyphrase_query": "fine-grained categorization image", "abstract": "Semantic part localization can facilitate fine-grained categorization by explicitly isolating subtle appearance differences associated with specific object parts. Methods for pose-normalized representations have been proposed, but generally presume bounding box annotations at test time due to the difficulty of object detection. We propose a model for fine-grained categorization that overcomes these limitations by leveraging deep convolutional features computed on bottom-up region proposals. Our method learns whole-object and part detectors, enforces learned geometric constraints between them, and predicts a fine-grained category from a pose-normalized representation. Experiments on the Caltech-UCSD bird dataset confirm that our method outperforms state-of-the-art fine-grained categorization methods in an end-to-end evaluation without requiring a bounding box at test time.", "task": "fine-grained categorization", "domain": "", "modality": "Image", "language": "", "training_style": "Unsupervised", "text_length": "", "query": "Model for fine-grained bird category prediction from pose normalized representations using deep convolutional features over bottom up region proposals, and learned object and part detectors, and geometric constraints."}
{"documents": ["CoNLL-2014 Shared Task: Grammatical Error Correction", "FCE"], "year": 2017, "keyphrase_query": "error detection text", "abstract": "Shortage of available training data is holding back progress in the area of automated error detection. This paper investigates two alternative methods for artificially generating writing errors, in order to create additional resources. We propose treating error generation as a machine translation task, where grammatically correct text is translated to contain errors. In addition, we explore a system for extracting textual patterns from an annotated corpus, which can then be used to insert errors into grammatically correct sentences. Our experiments show that the inclusion of artificially generated errors significantly improves error detection accuracy on both FCE and CoNLL 2014 datasets.", "task": "error detection", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "Sentence-level", "query": "Machine translation formulation and system for extracting textual patterns for creation of artificial data to improve error detection."}
{"documents": ["Omniglot"], "year": 2017, "keyphrase_query": "meta-learning, few-shot image classification, regression, reinforcement", "abstract": "We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.", "task": "meta-learning, few-shot image classification, few-shot regression, reinforcement learning", "domain": "", "modality": "", "language": "", "training_style": "Few-shot", "text_length": "", "query": "Model-agnostic meta-learning for few shot generalization"}
{"documents": ["Criteo"], "year": 2017, "keyphrase_query": "ctr prediction raw features", "abstract": "Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide & Deep model from Google, DeepFM has a shared input to its \"wide\" and \"deep\" parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data.", "task": "CTR prediction", "domain": "", "modality": "raw features", "language": "", "training_style": "", "text_length": "", "query": "We propose a new model for learning sophisticated feature interactions behind user behaviors."}
{"documents": ["WMT 2015"], "year": 2015, "keyphrase_query": "neural machine translation text", "abstract": "Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English!German and English!Russian by up to 1.1 and 1.3 BLEU, respectively.", "task": "neural machine translation", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We introduce a new approach to neural machine translation which encodes rare and unknown words as sequences of sub-word units."}
{"documents": ["Multi-Domain Sentiment Dataset v2.0"], "year": 2018, "keyphrase_query": "distributional correspondence indexing text", "abstract": "This paper introduces PyDCI, a new implementation of Distributional Correspondence Indexing (DCI) written in Python. DCI is a transfer learning method for cross-domain and cross-lingual text classification for which we had provided an implementation (here called JaDCI) built on top of JaTeCS, a Java framework for text classification. PyDCI is a stand-alone version of DCI that exploits scikit-learn and the SciPy stack. We here report on new experiments that we have carried out in order to test PyDCI, and in which we use as baselines new high-performing methods that have appeared after DCI was originally proposed. These experiments show that, thanks to a few subtle ways in which we have improved DCI, PyDCI outperforms both JaDCI and the above-mentioned high-performing methods, and delivers the best known results on the two popular benchmarks on which we had tested DCI, i.e., MultiDomainSentiment (a.k.a. MDS -- for cross-domain adaptation) and Webis-CLS-10 (for cross-lingual adaptation). PyDCI, together with the code allowing to replicate our experiments, is available at https://github.com/AlexMoreo/pydci .", "task": "Distributional Correspondence Indexing", "domain": "", "modality": "Text", "language": "", "training_style": "Transfer learning", "text_length": "", "query": "We introduce a transfer learning method for cross-domain and cross-lingual text classification."}
{"documents": ["BUCC", "United Nations Parallel Corpus"], "year": 2018, "keyphrase_query": "machine translation text english, french, german", "abstract": "Machine translation is highly sensitive to the size and quality of the training data, which has led to an increasing interest in collecting and filtering large parallel corpora. In this paper, we propose a new method for this task based on multilingual sentence embeddings. Our approach uses an encoder-decoder trained over an initial parallel corpus to build multilingual sentence representations, which are then incorporated into a new margin-based method to score, mine and filter parallel sentences. In contrast to previous approaches, which rely on nearest neighbor retrieval with a hard threshold over cosine similarity, our proposed method accounts for the scale inconsistencies of this measure, considering the margin between a given sentence pair and its closest candidates instead. Our experiments show large improvements over existing methods. We outperform the best published results on the BUCC shared task on parallel corpus mining by more than 10 F1 points. We also improve the precision from 48.9 to 83.3 on the reconstruction of 11.3M English-French sentence pairs of the UN corpus. Finally, filtering the English-German ParaCrawl corpus with our approach, we obtain 31.2 BLEU points on newstest2014, an improvement of more than one point over the best official filtered version.", "task": "machine translation", "domain": "", "modality": "Text", "language": "English, French, German", "training_style": "Supervised training/finetuning", "text_length": "Sentence-level", "query": "We propose a new approach to multilingual translation based on multilingual sentence embeddings."}
{"documents": ["WMT 2014", "WMT 2016"], "year": 2017, "keyphrase_query": "machine translation text", "abstract": "Existing approaches to neural machine translation condition each output word on previously generated outputs. We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference. Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher. We demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy, and validate our approach on IWSLT 2016 English-German and two WMT language pairs. By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English-Romanian.", "task": "machine translation", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "Sentence-level, paragraph-level", "query": "We propose a machine translation model that avoids the autoregressive property and produces outputs in parallel, allowing an order of magnitude lower latency during inference."}
{"documents": ["Switchboard-1 Corpus"], "year": 2016, "keyphrase_query": "conversational speech recognition", "abstract": "Conversational speech recognition has served as a flagship speech recognition task since the release of the Switchboard corpus in the 1990s. In this paper, we measure the human error rate on the widely used NIST 2000 test set, and find that our latest automated system has reached human parity. The error rate of professional transcribers is 5.9% for the Switchboard portion of the data, in which newly acquainted pairs of people discuss an assigned topic, and 11.3% for the CallHome portion where friends and family members have open-ended conversations. In both cases, our automated system establishes a new state of the art, and edges past the human benchmark, achieving error rates of 5.8% and 11.0%, respectively. The key to our system's performance is the use of various convolutional and LSTM acoustic model architectures, combined with a novel spatial smoothing method and lattice-free MMI acoustic training, multiple recurrent neural network language modeling approaches, and a systematic use of system combination.", "task": "conversational speech recognition", "domain": "", "modality": "speech", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I developed a conversational speech recognition system using convolutional and LSTM acoustic model architectures that achieve lower error rates than human transcribers."}
{"documents": ["ASPEC", "IMDb Movie Reviews"], "year": 2017, "keyphrase_query": "text compression, machine translation, sentiment analysis,", "abstract": "Natural language processing (NLP) models often require a massive number of parameters for word embeddings, resulting in a large storage or memory footprint. Deploying neural NLP models to mobile devices requires compressing the word embeddings without any significant sacrifices in performance. For this purpose, we propose to construct the embeddings with few basis vectors. For each word, the composition of basis vectors is determined by a hash code. To maximize the compression rate, we adopt the multi-codebook quantization approach instead of binary coding scheme. Each code is composed of multiple discrete numbers, such as (3, 2, 1, 8), where the value of each component is limited to a fixed range. We propose to directly learn the discrete codes in an end-to-end neural network by applying the Gumbel-softmax trick. Experiments show the compression rate achieves 98% in a sentiment analysis task and 94% ~ 99% in machine translation tasks without performance loss. In both tasks, the proposed method can improve the model performance by slightly lowering the compression rate. Compared to other approaches such as character-level segmentation, the proposed method is language-independent and does not require modifications to the network architecture.", "task": "text compression, machine translation, sentiment analysis, ", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I propose a multi-codebook quantization-based method for compressing word embeddings."}
{"documents": ["CoNLL-2012"], "year": 2019, "keyphrase_query": "semantic role labeling text", "abstract": "Semantic role labeling (SRL) aims to discover the predicateargument structure of a sentence. End-to-end SRL without syntactic input has received great attention. However, most of them focus on either span-based or dependency-based semantic representation form and only show specific model optimization respectively. Meanwhile, handling these two SRL tasks uniformly was less successful. This paper presents an end-to-end model for both dependency and span SRL with a unified argument representation to deal with two different types of argument annotations in a uniform fashion. Furthermore, we jointly predict all predicates and arguments, especially including long-term ignored predicate identification subtask. Our single model achieves new state-of-the-art results on both span (CoNLL 2005, 2012) and dependency (CoNLL 2008, 2009) SRL benchmarks.", "task": "semantic role labeling", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "Sentence-level", "query": "I want to design a single unified model for both span-based and dependency-based input representations for semantic role labeling"}
{"documents": ["WMT 2014", "WMT 2016 News"], "year": 2016, "keyphrase_query": "machine translation text english, czech, german, romanian, russian", "abstract": "We participated in the WMT 2016 shared news translation task by building neural translation systems for four language pairs, each trained in both directions: English↔Czech, English↔German, English↔Romanian and English↔Russian. Our systems are based on an attentional encoder-decoder, using BPE subword segmentation for open-vocabulary translation with a fixed vocabulary. We experimented with using automatic back-translations of the monolingual News corpus as additional training data, pervasive dropout, and target-bidirectional models. All reported methods give substantial improvements, and we see improvements of 4.3–11.2 BLEU over our baseline systems. In the human evaluation, our systems were the (tied) best constrained system for 7 out of 8 translation directions in which we participated.12", "task": "machine translation", "domain": "", "modality": "Text", "language": "English, Czech, German, Romanian, Russian", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to build a machine translation system for news articles in english, czech, german, romanian, and russian."}
{"documents": ["PASCAL Context", "PASCAL VOC"], "year": 2015, "keyphrase_query": "semantic segmentation image", "abstract": "We present a technique for adding global context to deep convolutional networks for semantic segmentation. The approach is simple, using the average feature for a layer to augment the features at each location. In addition, we study several idiosyncrasies of training, significantly increasing the performance of baseline networks (e.g. from FCN). When we add our proposed global feature, and a technique for learning normalization parameters, accuracy increases consistently even over our improved versions of the baselines. Our proposed approach, ParseNet, achieves state-of-the-art performance on SiftFlow and PASCAL-Context with small additional computational cost over baselines, and near current state-of-the-art performance on PASCAL VOC 2012 semantic segmentation with a simple approach. Code is available at this https URL .", "task": "semantic segmentation", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "We propose a method for adding global context into semantic segmentation models based on convolutional neural networks."}
{"documents": ["CNN/Daily Mail"], "year": 2016, "keyphrase_query": "document comprehension text news english", "abstract": "Enabling a computer to understand a document so that it can answer comprehension questions is a central, yet unsolved goal of NLP. A key factor impeding its solution by machine learned systems is the limited availability of human-annotated data. Hermann et al. (2015) seek to solve this problem by creating over a million training examples by pairing CNN and Daily Mail news articles with their summarized bullet points, and show that a neural network can then be trained to give good performance on this task. In this paper, we conduct a thorough examination of this new reading comprehension task. Our primary aim is to understand what depth of language understanding is required to do well on this task. We approach this from one side by doing a careful hand-analysis of a small subset of the problems and from the other by showing that simple, carefully designed systems can obtain accuracies of 73.6% and 76.6% on these two datasets, exceeding current state-of-the-art results by 7-10% and approaching what we believe is the ceiling for performance on this task.", "task": "Document Comprehension", "domain": "News", "modality": "Text", "language": "English", "training_style": "Supervised training/finetuning", "text_length": "Paragraph-level", "query": "We investigate what levels of document understanding are required for the novel CNN and Daily News reading comprehension task."}
{"documents": ["PASCAL VOC 2007"], "year": 2016, "keyphrase_query": "object detection image", "abstract": "Weakly supervised learning of object detection is an important problem in image understanding that still does not have a satisfactory solution. In this paper, we address this problem by exploiting the power of deep convolutional neural networks pre-trained on large-scale image-level classification tasks. We propose a weakly supervised deep detection architecture that modifies one such network to operate at the level of image regions, performing simultaneously region selection and classification. Trained as an image classifier, the architecture implicitly learns object detectors that are better than alternative weakly supervised detection systems on the PASCAL VOC data. The model, which is a simple and elegant end-to-end architecture, outperforms standard data augmentation and fine-tuning techniques for the task of image-level classification as well.", "task": "object detection", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We propose a weakly supervised deep detection architecture that modifies one such network to operate at the level of image regions, performing simultaneously region selection and classification. "}
{"documents": ["YCB-Video"], "year": 2017, "keyphrase_query": "6d object pose estimation video robotics", "abstract": "Estimating the 6D pose of known objects is important for robots to interact with the real world. The problem is challenging due to the variety of objects as well as the complexity of a scene caused by clutter and occlusions between objects. In this work, we introduce PoseCNN, a new Convolutional Neural Network for 6D object pose estimation. PoseCNN estimates the 3D translation of an object by localizing its center in the image and predicting its distance from the camera. The 3D rotation of the object is estimated by regressing to a quaternion representation. We also introduce a novel loss function that enables PoseCNN to handle symmetric objects. In addition, we contribute a large scale video dataset for 6D object pose estimation named the YCB-Video dataset. Our dataset provides accurate 6D poses of 21 objects from the YCB dataset observed in 92 videos with 133,827 frames. We conduct extensive experiments on our YCB-Video dataset and the OccludedLINEMOD dataset to show that PoseCNN is highly robust to occlusions, can handle symmetric objects, and provide accurate pose estimation using only color images as input. When using depth data to further refine the poses, our approach achieves state-of-the-art results on the challenging OccludedLINEMOD dataset. Our code and dataset are available at this https URL.", "task": "6D Object Pose Estimation", "domain": "Robotics", "modality": "Video", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We introduce PoseCNN, a new Convolutional Neural Network for 6D object pose estimation."}
{"documents": ["SMS-WSJ", "Switchboard-1 Corpus"], "year": 2014, "keyphrase_query": "speech recognition", "abstract": "We present a state-of-the-art speech recognition system developed using end-to-end deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a \"phoneme.\" Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called Deep Speech, outperforms previously published results on the widely studied Switchboard Hub5'00, achieving 16.0% error on the full test set. Deep Speech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems.", "task": "speech recognition", "domain": "", "modality": "Speech", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We present a novel end-to-end deep learning approach for speech recognition."}
{"documents": ["TIMIT"], "year": 2015, "keyphrase_query": "speech recognition", "abstract": "Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks including machine translation, handwriting synthesis [1,2] and image caption generation [3]. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in [2] reaches a competitive 18.7% phoneme error rate (PER) on the TIMET phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18% PER in single utterances and 20% in 10-times longer (repeated) utterances. Finally, we propose a change to the attention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6% level.", "task": "speech recognition", "domain": "", "modality": "speech", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We want to build a model for speech recognition on longer recordings."}
{"documents": ["DUC 2004"], "year": 2016, "keyphrase_query": "text summarization", "abstract": "In this work, we cast text summarization as a sequence-to-sequence problem and apply the attentional encoder-decoder RNN that has been shown to be successful for Machine Translation (Bahdanau et al. (2014)). Our experiments show that the proposed architecture significantly outperforms the state-of-the art model of Rush et al. (2015) on the Gigaword dataset without any additional tuning. We also propose additional extensions to the standard architecture, which we show contribute to further improvement in performance.", "task": "text summarization", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "Paragraph-level", "query": "Attentional encoder-decoder network applied to text summarization"}
{"documents": ["CNN/Daily Mail"], "year": 2015, "keyphrase_query": "machine reading text", "abstract": "Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.", "task": "Machine reading", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "Paragraph-level", "query": "I want to train a supervised model for reading comprehension and then answering complex questions."}
{"documents": ["Visual Question Answering"], "year": 2016, "keyphrase_query": "visual question answering image text", "abstract": "Deep neural networks continue to advance the state-of-the-art of image recognition tasks with various methods. However, applications of these methods to multimodality remain limited. We present Multimodal Residual Networks (MRN) for the multimodal residual learning of visual question-answering, which extends the idea of the deep residual learning. Unlike the deep residual learning, MRN effectively learns the joint representation from vision and language information. The main idea is to use element-wise multiplication for the joint residual mappings exploiting the residual learning of the attentional models in recent studies. Various alternative models introduced by multimodality are explored based on our study. We achieve the state-of-the-art results on the Visual QA dataset for both Open-Ended and Multiple-Choice tasks. Moreover, we introduce a novel method to visualize the attention effect of the joint representations for each learning block using back-propagation algorithm, even though the visual features are collapsed without spatial information.", "task": "visual question answering", "domain": "", "modality": "image and text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to train a multimodal network to jointly learn visual and language representations from images and text for visual question answering."}
{"documents": ["SK-LARGE"], "year": 2019, "keyphrase_query": "object skeleton detection image", "abstract": "Computing object skeletons in natural images is challenging, owing to large variations in object appearance and scale, and the complexity of handling background clutter. Many recent methods frame object skeleton detection as a binary pixel classification problem, which is similar in spirit to learning-based edge detection, as well as to semantic segmentation methods. In the present article, we depart from this strategy by training a CNN to predict a two-dimensional vector field, which maps each scene point to a candidate skeleton pixel, in the spirit of flux-based skeletonization algorithms. This ``image context flux'' representation has two major advantages over previous approaches. First, it explicitly encodes the relative position of skeletal pixels to semantically meaningful entities, such as the image points in their spatial context, and hence also the implied object boundaries. Second, since the skeleton detection context is a region-based vector field, it is better able to cope with object parts of large width. We evaluate the proposed method on three benchmark datasets for skeleton detection and two for symmetry detection, achieving consistently superior performance over state-of-the-art methods.", "task": "object skeleton detection", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "This is a method for object skeleton detection using flux-based skeletonization algorithms."}
{"documents": ["MNIST", "SVHN"], "year": 2015, "keyphrase_query": "semi-supervised classification, image generation", "abstract": "In this paper, we propose the\"adversarial autoencoder\"(AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.", "task": "semi-supervised classification, image generation", "domain": "", "modality": "Image", "language": "", "training_style": "Semi-supervised", "text_length": "", "query": "I want to train a generative model using the adversarial autoencoder for semi-supervised classification and image generation."}
{"documents": ["IMDb Movie Reviews", "RCV1"], "year": 2014, "keyphrase_query": "text categorization", "abstract": "Convolutional neural network (CNN) is a neural network that can make use of the internal structure of data such as the 2D structure of image data. This paper studies CNN on text categorization to exploit the 1D structure (namely, word order) of text data for accurate prediction. Instead of using low-dimensional word vectors as input as is often done, we directly apply CNN to high-dimensional text data, which leads to directly learning embedding of small text regions for use in classification. In addition to a straightforward adaptation of CNN from image to text, a simple but new variation which employs bag-of-word conversion in the convolution layer is proposed. An extension to combine multiple convolution layers is also explored for higher accuracy. The experiments demonstrate the effectiveness of our approach in comparison with state-of-the-art methods.", "task": "text categorization", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to train a convolutional neural network on text categorization to exploit the 1D structure"}
{"documents": ["MS MARCO"], "year": 2019, "keyphrase_query": "query-based passage re-ranking text", "abstract": "Recently, neural models pretrained on a language modeling task, such as ELMo (Peters et al., 2017), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2018), have achieved impressive results on various natural language processing tasks such as question-answering and natural language inference. In this paper, we describe a simple re-implementation of BERT for query-based passage re-ranking. Our system is the state of the art on the TREC-CAR dataset and the top entry in the leaderboard of the MS MARCO passage retrieval task, outperforming the previous state of the art by 27% (relative) in MRR@10. The code to reproduce our results is available at this https URL", "task": "query-based passage re-ranking", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "Paragraph-level", "query": "I proposed a simple re-implementation of BERT for query-based passage re-ranking."}
{"documents": ["VisDial", "Visual Genome", "Visual Question Answering v2.0"], "year": 2018, "keyphrase_query": "visual question answering image text", "abstract": "This document describes Pythia v0.1, the winning entry from Facebook AI Research (FAIR)'s A-STAR team to the VQA Challenge 2018. ::: Our starting point is a modular re-implementation of the bottom-up top-down (up-down) model. We demonstrate that by making subtle but important changes to the model architecture and the learning rate schedule, fine-tuning image features, and adding data augmentation, we can significantly improve the performance of the up-down model on VQA v2.0 dataset -- from 65.67% to 70.22%. ::: Furthermore, by using a diverse ensemble of models trained with different features and on different datasets, we are able to significantly improve over the 'standard' way of ensembling (i.e. same model with different random seeds) by 1.31%. Overall, we achieve 72.27% on the test-std split of the VQA v2.0 dataset. Our code in its entirety (training, evaluation, data-augmentation, ensembling) and pre-trained models are publicly available at: this https URL", "task": "visual question answering", "domain": "", "modality": "Image and text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "Sentence-level", "query": "We improve the performance of the up-down model on the for VQA."}
{"documents": ["BraTS 2018"], "year": 2018, "keyphrase_query": "semantic segmentation 3d volumetric biomedical", "abstract": "Automated segmentation of brain tumors from 3D magnetic resonance images (MRIs) is necessary for the diagnosis, monitoring, and treatment planning of the disease. Manual delineation practices require anatomical knowledge, are expensive, time consuming and can be inaccurate due to human error. Here, we describe a semantic segmentation network for tumor subregion segmentation from 3D MRIs based on encoder-decoder architecture. Due to a limited training dataset size, a variational auto-encoder branch is added to reconstruct the input image itself in order to regularize the shared decoder and impose additional constraints on its layers. The current approach won 1st place in the BraTS 2018 challenge.", "task": "semantic segmentation", "domain": "biomedical", "modality": "3D volumetric", "language": "", "training_style": "Semi-supervised", "text_length": "", "query": "a semantic segmentation network for tumor subregion segmentation from 3D MRIs based on encoder-decoder architecture"}
{"documents": ["WMT 2014"], "year": 2017, "keyphrase_query": "neural machine translation text language english, german, french", "abstract": "State-of-the-art results on neural machine translation often use attentional sequence-to-sequence models with some form of convolution or recursion. Vaswani et. al. (2017) propose a new architecture that avoids recurrence and convolution completely. Instead, it uses only self-attention and feed-forward layers. While the proposed architecture achieves state-of-the-art results on several machine translation tasks, it requires a large number of parameters and training iterations to converge. We propose Weighted Transformer, a Transformer with modified attention layers, that not only outperforms the baseline network in BLEU score but also converges 15-40% faster. Specifically, we replace the multi-head attention by multiple self-attention branches that the model learns to combine during the training process. Our model improves the state-of-the-art performance by 0.5 BLEU points on the WMT 2014 English-to-German translation task and by 0.4 on the English-to-French translation task.", "task": "neural machine translation", "domain": "language", "modality": "Text", "language": "English, German, French", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We propose a Transformer model with modified attention layers for machine translation."}
{"documents": ["WMT 2014"], "year": 2014, "keyphrase_query": "translation text english, french", "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.", "task": "translation", "domain": "", "modality": "Text", "language": "English, French", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We present a general end-to-end approach to sequence learning that uses a multilayered Long Short-Term Memory (LSTM)."}
{"documents": ["MPII Human Pose"], "year": 2017, "keyphrase_query": "multi-person pose estimation", "abstract": "This paper proposes a new Generative Partition Network (GPN) to address the challenging multi-person pose estimation problem. Different from existing models that are either completely top-down or bottom-up, the proposed GPN introduces a novel strategy--it generates partitions for multiple persons from their global joint candidates and infers instance-specific joint configurations simultaneously. The GPN is favorably featured by low complexity and high accuracy of joint detection and re-organization. In particular, GPN designs a generative model that performs one feed-forward pass to efficiently generate robust person detections with joint partitions, relying on dense regressions from global joint candidates in an embedding space parameterized by centroids of persons. In addition, GPN formulates the inference procedure for joint configurations of human poses as a graph partition problem, and conducts local optimization for each person detection with reliable global affinity cues, leading to complexity reduction and performance improvement. GPN is implemented with the Hourglass architecture as the backbone network to simultaneously learn joint detector and dense regressor. Extensive experiments on benchmarks MPII Human Pose Multi-Person, extended PASCAL-Person-Part, and WAF, show the efficiency of GPN with new state-of-the-art performance.", "task": "multi-person pose estimation", "domain": "", "modality": "", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I propose a method for multi-person pose estimation from global joint candidates."}
{"documents": ["Dialogue State Tracking Challenge", "Wizard-of-Oz"], "year": 2018, "keyphrase_query": "dialogue state tracking text", "abstract": "Dialogue state tracking, which estimates user goals and requests given the dialogue context, is an essential part of task-oriented dialogue systems. In this paper, we propose the Global-Locally Self-Attentive Dialogue State Tracker (GLAD), which learns representations of the user utterance and previous system actions with global-local modules. Our model uses global modules to share parameters between estimators for different types (called slots) of dialogue states, and uses local modules to learn slot-specific features. We show that this significantly improves tracking of rare states and achieves state-of-the-art performance on the WoZ and DSTC2 state tracking tasks. GLAD obtains 88.1% joint goal accuracy and 97.1% request accuracy on WoZ, outperforming prior work by 3.7% and 5.5%. On DSTC2, our model obtains 74.5% joint goal accuracy and 97.5% request accuracy, outperforming prior work by 1.1% and 1.0%.", "task": "dialogue state tracking", "domain": "dialogue", "modality": "Text", "language": "", "training_style": "", "text_length": "", "query": "We are building a dialogue state tracking model which learns representations of the user utterance and previous system actions for a task-oriented dialogue system."}
{"documents": ["PASCAL VOC 2011"], "year": 2014, "keyphrase_query": "keypoint prediction image", "abstract": "Convolutional neural nets (convnets) trained from massive labeled datasets [1] have substantially improved the state-of-the-art in image classification [2] and object detection [3]. However, visual understanding requires establishing correspondence on a finer level than object category. Given their large pooling regions and training from whole-image labels, it is not clear that convnets derive their success from an accurate correspondence model which could be used for precise localization. In this paper, we study the effectiveness of convnet activation features for tasks requiring correspondence. We present evidence that convnet features localize at a much finer scale than their receptive field sizes, that they can be used to perform intraclass aligment as well as conventional hand-engineered features, and that they outperform conventional features in keypoint prediction on objects from PASCAL VOC 2011 [4].", "task": "keypoint prediction", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "We study the effectiveness of convnet activation features for tasks requiring correspondence on a finer level than object category, such as keypoint prediction."}
{"documents": ["WMT 2014", "WMT 2016"], "year": 2019, "keyphrase_query": "machine translation text english, german", "abstract": "While machine translation has traditionally relied on large amounts of parallel corpora, a recent research line has managed to train both Neural Machine Translation (NMT) and Statistical Machine Translation (SMT) systems using monolingual corpora only. In this paper, we identify and address several deficiencies of existing unsupervised SMT approaches by exploiting subword information, developing a theoretically well founded unsupervised tuning method, and incorporating a joint refinement procedure. Moreover, we use our improved SMT system to initialize a dual NMT model, which is further fine-tuned through on-the-fly back-translation. Together, we obtain large improvements over the previous state-of-the-art in unsupervised machine translation. For instance, we get 22.5 BLEU points in English-to-German WMT 2014, 5.5 points more than the previous best unsupervised system, and 0.5 points more than the (supervised) shared task winner back in 2014.", "task": "machine translation", "domain": "", "modality": "Text", "language": "English, German", "training_style": "Unsupervised", "text_length": "", "query": "By exploiting subword information, we develop a theoretically well founded unsupervised tuning method, and incorporate a joint refinement procedure for machine translation."}
{"documents": ["MNIST", "Omniglot"], "year": 2016, "keyphrase_query": "learning representations datasets", "abstract": "An efficient learner is one who reuses what they already know to tackle a new problem. For a machine learner, this means understanding the similarities amongst datasets. In order to do this, one must take seriously the idea of working with datasets, rather than datapoints, as the key objects to model. Towards this goal, we demonstrate an extension of a variational autoencoder that can learn a method for computing representations, or statistics, of datasets in an unsupervised fashion. The network is trained to produce statistics that encapsulate a generative model for each dataset. Hence the network enables efficient learning from new datasets for both unsupervised and supervised tasks. We show that we are able to learn statistics that can be used for: clustering datasets, transferring generative models to new datasets, selecting representative samples of datasets and classifying previously unseen classes.", "task": "learning representations of datasets", "domain": "", "modality": "", "language": "", "training_style": "Unsupervised", "text_length": "", "query": "I would like a dataset for unsupervised learning of dataset representations useful for supervised and unsupervised downstream tasks."}
{"documents": ["WMT 2014", "WMT 2016"], "year": 2017, "keyphrase_query": "english-german translation, english-french text english, german,", "abstract": "The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.", "task": "English-German translation, English-French translation", "domain": "", "modality": "Text", "language": "English, German, French", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I would like a dataset for supervised machine translation from english to german and english to french."}
{"documents": ["ICDAR 2017", "SCUT-CTW1500"], "year": 2019, "keyphrase_query": "scene text detection image", "abstract": "Scene text detection has witnessed rapid progress especially with the recent development of convolutional neural networks. However, there still exists two challenges which prevent the algorithm into industry applications. On the one hand, most of the state-of-art algorithms require quadrangle bounding box which is in-accurate to locate the texts with arbitrary shape. On the other hand, two text instances which are close to each other may lead to a false detection which covers both instances. Traditionally, the segmentation-based approach can relieve the first problem but usually fail to solve the second challenge. To address these two challenges, in this paper, we propose a novel Progressive Scale Expansion Network (PSENet), which can precisely detect text instances with arbitrary shapes. More specifically, PSENet generates the different scale of kernels for each text instance, and gradually expands the minimal scale kernel to the text instance with the complete shape. Due to the fact that there are large geometrical margins among the minimal scale kernels, our method is effective to split the close text instances, making it easier to use segmentation-based methods to detect arbitrary-shaped text instances. Extensive experiments on CTW1500, Total-Text, ICDAR 2015 and ICDAR 2017 MLT validate the effectiveness of PSENet. Notably, on CTW1500, a dataset full of long curve texts, PSENet achieves a F-measure of 74.3% at 27 FPS, and our best F-measure (82.2%) outperforms state-of-art algorithms by 6.6%. The code will be released in the future.", "task": "scene text detection", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "Dataset for scene text recognition with arbitrary-shaped text instances"}
{"documents": ["Hutter Prize", "WMT 2014", "WMT 2015 News"], "year": 2016, "keyphrase_query": "sequence processing text", "abstract": "We present a novel neural network for processing sequences. The ByteNet is a one-dimensional convolutional neural network that is composed of two parts, one to encode the source sequence and the other to decode the target sequence. The two network parts are connected by stacking the decoder on top of the encoder and preserving the temporal resolution of the sequences. To address the differing lengths of the source and the target, we introduce an efficient mechanism by which the decoder is dynamically unfolded over the representation of the encoder. The ByteNet uses dilation in the convolutional layers to increase its receptive field. The resulting network has two core properties: it runs in time that is linear in the length of the sequences and it sidesteps the need for excessive memorization. The ByteNet decoder attains state-of-the-art performance on character-level language modelling and outperforms the previous best results obtained with recurrent networks. The ByteNet also achieves state-of-the-art performance on character-to-character machine translation on the English-to-German WMT translation task, surpassing comparable neural translation models that are based on recurrent networks with attentional pooling and run in quadratic time. We find that the latent alignment structure contained in the representations reflects the expected alignment between the tokens.", "task": "sequence processing", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "", "query": "We present a novel neural network for sequence-to-sequence modeling."}
{"documents": ["Visual Question Answering"], "year": 2017, "keyphrase_query": "visual question answering images text", "abstract": "This paper proposes to improve visual question answering (VQA) with structured representations of both scene contents and questions. A key challenge in VQA is to require joint reasoning over the visual and text domains. The predominant CNN/LSTM-based approach to VQA is limited by monolithic vector representations that largely ignore structure in the scene and in the form of the question. CNN feature vectors cannot effectively capture situations as simple as multiple object instances, and LSTMs process questions as series of words, which does not reflect the true complexity of language structure. We instead propose to build graphs over the scene objects and over the question words, and we describe a deep neural network that exploits the structure in these representations. This shows significant benefit over the sequential processing of LSTMs. The overall efficacy of our approach is demonstrated by significant improvements over the state-of-the-art, from 71.2% to 74.4% in accuracy on the \"abstract scenes\" multiple-choice benchmark, and from 34.7% to 39.1% in accuracy over pairs of \"balanced\" scenes, i.e. images with fine-grained differences and opposite yes/no answers to a same question.", "task": "visual question answering", "domain": "", "modality": "images and text", "language": "", "training_style": "", "text_length": "", "query": "We want to improve visual question answering (VQA) by leveraging graph representations of both scene contents and questions."}
{"documents": ["Switchboard-1 Corpus"], "year": 2016, "keyphrase_query": "speech recognition low-resource", "abstract": "Abstract : Convolutional neural networks (CNNs) are a standard component of many current state-of-the-art Large Vocabulary Continuous Speech Recognition (LVCSR) systems. However, CNNs in LVCSR have not kept pace with recent advances in other domains where deeper neural networks provide superior performance. In this paper we propose a number of architectural advances in CNNs for LVCSR. First, we introduce a very deep convolutional network architecture with up to 14 weight layers. There are multiple convolutional layers before each pooling layer, with small 3x3 kernels, inspired by the VGG Imagenet 2014 architecture. Then, we introduce multilingual CNNs with multiple untied layers. Finally, we introduce multi-scale input features aimed at exploiting more context at negligible computational cost. We evaluate the improvements first on a Babel task for low resource speech recognition, obtaining an absolute 5.77 WER improvement over the baseline PLP DNN by training our CNN on the combined data of six different languages. We then evaluate the very deep CNNs on the Hub500 benchmark (using the 262 hours of SWB-1 training data) achieving a word error rate of 11.8 after cross-entropy training, a 1.4 WER improvement (10.6 relative) over the best published CNN result so far.", "task": "speech recognition", "domain": "low-resource", "modality": "Speech", "language": "", "training_style": "", "text_length": "", "query": "We propose a system for multilingual speech recognition that can perform in both low-resource and high-resource settings."}
{"documents": ["TIMIT"], "year": 2018, "keyphrase_query": "speech recognition", "abstract": "Recently, the connectionist temporal classification (CTC) model coupled with recurrent (RNN) or convolutional neural networks (CNN), made it easier to train speech recognition systems in an end-to-end fashion. However in real-valued models, time frame components such as mel-filter-bank energies and the cepstral coefficients obtained from them, together with their first and second order derivatives, are processed as individual elements, while a natural alternative is to process such components as composed entities. We propose to group such elements in the form of quaternions and to process these quaternions using the established quaternion algebra. Quaternion numbers and quaternion neural networks have shown their efficiency to process multidimensional inputs as entities, to encode internal dependencies, and to solve many tasks with less learning parameters than real-valued models. This paper proposes to integrate multiple feature views in quaternion-valued convolutional neural network (QCNN), to be used for sequence-to-sequence mapping with the CTC model. Promising results are reported using simple QCNNs in phoneme recognition experiments with the TIMIT corpus. More precisely, QCNNs obtain a lower phoneme error rate (PER) with less learning parameters than a competing model based on real-valued CNNs.", "task": "speech recognition", "domain": "", "modality": "Speech", "language": "", "training_style": "", "text_length": "", "query": "We want to reduce the phoneme error rate of speech recognition models."}
{"documents": ["BraTS 2013"], "year": 2017, "keyphrase_query": "segmentation glioblastoma mri biomedical", "abstract": "Abstract In this paper, we present a fully automatic brain tumor segmentation method based on Deep Neural Networks (DNNs). The proposed networks are tailored to glioblastomas (both low and high grade) pictured in MR images. By their very nature, these tumors can appear anywhere in the brain and have almost any kind of shape, size, and contrast. These reasons motivate our exploration of a machine learning solution that exploits a flexible, high capacity DNN while being extremely efficient. Here, we give a description of different model choices that we’ve found to be necessary for obtaining competitive performance. We explore in particular different architectures based on Convolutional Neural Networks (CNN), i.e. DNNs specifically adapted to image data. We present a novel CNN architecture which differs from those traditionally used in computer vision. Our CNN exploits both local features as well as more global contextual features simultaneously. Also, different from most traditional uses of CNNs, our networks use a final layer that is a convolutional implementation of a fully connected layer which allows a 40 fold speed up. We also describe a 2-phase training procedure that allows us to tackle difficulties related to the imbalance of tumor labels. Finally, we explore a cascade architecture in which the output of a basic CNN is treated as an additional source of information for a subsequent CNN. Results reported on the 2013 BRATS test data-set reveal that our architecture improves over the currently published state-of-the-art while being over 30 times faster.", "task": "segmentation of glioblastoma", "domain": "biomedical", "modality": "MRI", "language": "", "training_style": "", "text_length": "", "query": "A method for segmentation of glioblastoma to handle highly variable tumors in MRI scans."}
{"documents": ["CoNLL-2003"], "year": 2018, "keyphrase_query": "named entity recognition text", "abstract": "Conventional wisdom is that hand-crafted features are redundant for deep learning models, as they already learn adequate representations of text automatically from corpora. In this work, we test this claim by proposing a new method for exploiting handcrafted features as part of a novel hybrid learning approach, incorporating a feature auto-encoder loss component. We evaluate on the task of named entity recognition (NER), where we show that including manual features for part-of-speech, word shapes and gazetteers can improve the performance of a neural CRF model. We obtain a $F_1$ of 91.89 for the CoNLL-2003 English shared task, which significantly outperforms a collection of highly competitive baseline models. We also present an ablation study showing the importance of auto-encoding, over using features as either inputs or outputs alone, and moreover, show including the autoencoder components reduces training requirements to 60\\%, while retaining the same predictive accuracy.", "task": "named entity recognition", "domain": "", "modality": "Text", "language": "", "training_style": "Semi-supervised", "text_length": "", "query": "We want to investigate the effectiveness of handcrafted features like part-of-speech, word shapes, and gazetteers for the task of named entity recognition (NER)."}
{"documents": ["Billion Word Benchmark"], "year": 2016, "keyphrase_query": "language modeling text", "abstract": "In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7. We also release these models for the NLP and ML community to study and improve upon.", "task": "language modeling", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "", "query": "An language modeling approach based on character-level neural networks."}
{"documents": ["PASCAL VOC 2007"], "year": 2017, "keyphrase_query": "weakly-supervised object detection image", "abstract": "Weakly-supervised object detection (WOD) is a challenging problems in computer vision. The key problem is to simultaneously infer the exact object locations in the training images and train the object detectors, given only the training images with weak image-level labels. Intuitively, by simulating the selective attention mechanism of human visual system, saliency detection technique can select attractive objects in scenes and thus is a potential way to provide useful priors for WOD. However, the way to adopt saliency detection in WOD is not trivial since the detected saliency region might be possibly highly ambiguous in complex cases. To this end, this paper first comprehensively analyzes the challenges in applying saliency detection to WOD. Then, we make one of the earliest efforts to bridge saliency detection to WOD via the self-paced curriculum learning, which can guide the learning procedure to gradually achieve faithful knowledge of multi-class objects from easy to hard. The experimental results demonstrate that the proposed approach can successfully bridge saliency detection and WOD tasks and achieve the state-of-the-art object detection results under the weak supervision.", "task": "weakly-supervised object detection", "domain": "", "modality": "Image", "language": "", "training_style": "Weakly supervised", "text_length": "", "query": "I want to train an object detector under weak supervision of image-level labels."}
{"documents": ["2000 HUB5 English", "Switchboard-1 Corpus"], "year": 2016, "keyphrase_query": "automatic speech recognition text conversational data english", "abstract": "We describe a collection of acoustic and language modeling techniques that lowered the word error rate of our English conversational telephone LVCSR system to a record 6.6% on the Switchboard subset of the Hub5 2000 evaluation testset. On the acoustic side, we use a score fusion of three strong models: recurrent nets with maxout activations, very deep convolutional nets with 3x3 kernels, and bidirectional long short-term memory nets which operate on FMLLR and i-vector features. On the language modeling side, we use an updated model \"M\" and hierarchical neural network LMs.", "task": "automatic speech recognition", "domain": "conversational data", "modality": "speech and text", "language": "English", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to train an automatic speech recognizer system for conversational data in English."}
{"documents": ["CoNLL-2014 Shared Task: Grammatical Error Correction", "FCE"], "year": 2016, "keyphrase_query": "grammatical error detection text", "abstract": "In this paper, we present the first experiments using neural network models for the task of error detection in learner writing. We perform a systematic comparison of alternative compositional architectures and propose a framework for error detection based on bidirectional LSTMs. Experiments on the CoNLL-14 shared task dataset show the model is able to outperform other participants on detecting errors in learner writing. Finally, the model is integrated with a publicly deployed self-assessment system, leading to performance comparable to human annotators.", "task": "grammatical error detection", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We propose an LSTM-based model for grammatical error detection in learner writing."}
{"documents": ["Billion Word Benchmark"], "year": 2014, "keyphrase_query": "language model estimation text", "abstract": "We present a novel family of language model (LM) estimation techniques named Sparse Non-negative Matrix (SNM) estimation. A first set of experiments empirically evaluating it on the On e Billion Word Benchmark [Chelba et al., 2013] shows that SNM n-gram LMs perform almost as well as the well-established Kneser-Ney (KN) models. When using skip-gram features the models are able to match the state-of-the-art recurrent neural network (RNN) LMs; combining the two modeling techniques yields the best known result on the benchmark. The computational advantages of SNM over both maximum entropy and RNN LM estimation are probably its main strength, promising an approach that has the same flexibility in combining arbitrary feature s effectively and yet should scale to very large amounts of data as gracefully as n-gram LMs do.", "task": "Language model estimation", "domain": "", "modality": "Text", "language": "", "training_style": "Unsupervised", "text_length": "", "query": "I want to develop a self-supervised language model estimation method"}
{"documents": ["MNIST", "SVHN", "smallNORB"], "year": 2017, "keyphrase_query": "image classification, generation", "abstract": "In this paper, we describe the \"PixelGAN autoencoder\", a generative autoencoder in which the generative path is a convolutional autoregressive neural network on pixels (PixelCNN) that is conditioned on a latent code, and the recognition path uses a generative adversarial network (GAN) to impose a prior distribution on the latent code. We show that different priors result in different decompositions of information between the latent code and the autoregressive decoder. For example, by imposing a Gaussian distribution as the prior, we can achieve a global vs. local decomposition, or by imposing a categorical distribution as the prior, we can disentangle the style and content information of images in an unsupervised fashion. We further show how the PixelGAN autoencoder with a categorical prior can be directly used in semi-supervised settings and achieve competitive semi-supervised classification results on the MNIST, SVHN and NORB datasets.", "task": "image classification, image generation", "domain": "", "modality": "Image", "language": "", "training_style": "Semi-supervised", "text_length": "", "query": "We devise a model for unsupervised image style and content disentanglement and semi-supervised image classification."}
{"documents": ["TIMIT"], "year": 2013, "keyphrase_query": "speech recognition", "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.", "task": "speech recognition", "domain": "", "modality": "speech", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to build an end-to-end speech recognition model."}
{"documents": ["Nottingham"], "year": 2016, "keyphrase_query": "sequence generation discrete tokens", "abstract": "As a new way of training generative models, Generative Adversarial Nets (GAN) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real-valued data. However, it has limitations when the goal is for generating sequences of discrete tokens. A major reason lies in that the discrete outputs from the generative model make it difficult to pass the gradient update from the discriminative model to the generative model. Also, the discriminative model can only assess a complete sequence, while for a partially generated sequence, it is non-trivial to balance its current score and the future one once the entire sequence has been generated. In this paper, we propose a sequence generation framework, called SeqGAN, to solve the problems. Modeling the data generator as a stochastic policy in reinforcement learning (RL), SeqGAN bypasses the generator differentiation problem by directly performing gradient policy update. The RL reward signal comes from the GAN discriminator judged on a complete sequence, and is passed back to the intermediate state-action steps using Monte Carlo search. Extensive experiments on synthetic data and real-world tasks demonstrate significant improvements over strong baselines.", "task": "sequence generation", "domain": "", "modality": "discrete tokens", "language": "", "training_style": "Unsupervised", "text_length": "", "query": "We modify the GAN architecture to allow us to generate sequences of discrete tokens."}
{"documents": ["Dialogue State Tracking Challenge"], "year": 2017, "keyphrase_query": "question answering text", "abstract": "In this paper, we study the problem of question answering when reasoning over multiple facts is required. We propose Query-Reduction Network (QRN), a variant of Recurrent Neural Network (RNN) that effectively handles both short-term (local) and long-term (global) sequential dependencies to reason over multiple facts. QRN considers the context sentences as a sequence of state-changing triggers, and reduces the original query to a more informed query as it observes each trigger (context sentence) through time. Our experiments show that QRN produces the state-of-the-art results in bAbI QA and dialog tasks, and in a real goal-oriented dialog dataset. In addition, QRN formulation allows parallelization on RNN's time axis, saving an order of magnitude in time complexity for training and inference.", "task": "question answering", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "Paragraph-level", "query": "We want to build an RNN model for question-answering whose answers can reason over multiple facts across a sequence of input sentences."}
{"documents": ["MHP"], "year": 2018, "keyphrase_query": "multi-human parsing, person re-identification, dataset generation image object recognition, facial", "abstract": "Despite the noticeable progress in perceptual tasks like detection, instance segmentation and human parsing, computers still perform unsatisfactorily on visually understanding humans in crowded scenes, such as group behavior analysis, person re-identification and autonomous driving, etc. To this end, models need to comprehensively perceive the semantic information and the differences between instances in a multi-human image, which is recently defined as the multi-human parsing task. In this paper, we present a new large-scale database \"Multi-Human Parsing (MHP)\" for algorithm development and evaluation, and advances the state-of-the-art in understanding humans in crowded scenes. MHP contains 25,403 elaborately annotated images with 58 fine-grained semantic category labels, involving 2-26 persons per image and captured in real-world scenes from various viewpoints, poses, occlusion, interactions and background. We further propose a novel deep Nested Adversarial Network (NAN) model for multi-human parsing. NAN consists of three Generative Adversarial Network (GAN)-like sub-nets, respectively performing semantic saliency prediction, instance-agnostic parsing and instance-aware clustering. These sub-nets form a nested structure and are carefully designed to learn jointly in an end-to-end way. NAN consistently outperforms existing state-of-the-art solutions on our MHP and several other datasets, and serves as a strong baseline to drive the future research for multi-human parsing.", "task": "multi-human parsing, person re-identification, dataset generation", "domain": "object recognition, facial recognition", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We created a new large-scale dataset for a recently defined task, called \"multi-human parsing\" and introduce a nested adversarial network (NAN) based on GANs, to provide a strong baseline for performance on the new dataset."}
{"documents": ["2000 HUB5 English", "Switchboard-1 Corpus"], "year": 2016, "keyphrase_query": "conversational speech recognition audio", "abstract": "We describe Microsoft's conversational speech recognition system, in which we combine recent developments in neural-network-based acoustic and language modeling to advance the state of the art on the Switchboard recognition task. Inspired by machine learning ensemble techniques, the system uses a range of convolutional and recurrent neural networks. I-vector modeling and lattice-free MMI training provide significant gains for all acoustic model architectures. Language model rescoring with multiple forward and backward running RNNLMs, and word posterior-based system combination provide a 20% boost. The best single system uses a ResNet architecture acoustic model with RNNLM rescoring, and achieves a word error rate of 6.9% on the NIST 2000 Switchboard task. The combined system has an error rate of 6.2%, representing an improvement over previously reported results on this benchmark task.", "task": "conversational speech recognition", "domain": "", "modality": "audio", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We build a highly accurate speech recognition system."}
{"documents": ["MPII Human Pose"], "year": 2017, "keyphrase_query": "articulated tracking multiple people video", "abstract": "In this paper we propose an approach for articulated tracking of multiple people in unconstrained videos. Our starting point is a model that resembles existing architectures for single-frame pose estimation but is substantially faster. We achieve this in two ways: (1) by simplifying and sparsifying the body-part relationship graph and leveraging recent methods for faster inference, and (2) by offloading a substantial share of computation onto a feed-forward convolutional architecture that is able to detect and associate body joints of the same person even in clutter. We use this model to generate proposals for body joint locations and formulate articulated tracking as spatio-temporal grouping of such proposals. This allows to jointly solve the association problem for all people in the scene by propagating evidence from strong detections through time and enforcing constraints that each proposal can be assigned to one person only. We report results on a public MPII Human Pose benchmark and on a new MPII Video Pose dataset of image sequences with multiple people. We demonstrate that our model achieves state-of-the-art results while using only a fraction of time and is able to leverage temporal information to improve state-of-the-art for crowded scenes.", "task": "articulated tracking of multiple people", "domain": "", "modality": "Video", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I'm testing an approach for articulated tracking of multiple people in unconstrained videos."}
{"documents": ["Criteo"], "year": 2018, "keyphrase_query": "factorization models", "abstract": "Combinatorial features are essential for the success of many commercial models. Manually crafting these features usually comes with high cost due to the variety, volume and velocity of raw data in web-scale systems. Factorization based models, which measure interactions in terms of vector product, can learn patterns of combinatorial features automatically and generalize to unseen features as well. With the great success of deep neural networks (DNNs) in various fields, recently researchers have proposed several DNN-based factorization model to learn both low- and high-order feature interactions. Despite the powerful ability of learning an arbitrary function from data, plain DNNs generate feature interactions implicitly and at the bit-wise level. In this paper, we propose a novel Compressed Interaction Network (CIN), which aims to generate feature interactions in an explicit fashion and at the vector-wise level. We show that the CIN share some functionalities with convolutional neural networks (CNNs) and recurrent neural networks (RNNs). We further combine a CIN and a classical DNN into one unified model, and named this new model eXtreme Deep Factorization Machine (xDeepFM). On one hand, the xDeepFM is able to learn certain bounded-degree feature interactions explicitly; on the other hand, it can learn arbitrary low- and high-order feature interactions implicitly. We conduct comprehensive experiments on three real-world datasets. Our results demonstrate that xDeepFM outperforms state-of-the-art models. We have released the source code of xDeepFM at https://github.com/Leavingseason/xDeepFM.", "task": "factorization models", "domain": "", "modality": "", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I propose a novel Compressed Interaction Network (CIN), which aims to generate feature interactions in an explicit fashion and at the vector-wise level."}
{"documents": ["TrecQA"], "year": 2016, "keyphrase_query": "question answering text", "abstract": "As an alternative to question answering methods based on feature engineering, deep learning approaches such as convolutional neural networks (CNNs) and Long Short-Term Memory Models (LSTMs) have recently been proposed for semantic matching of questions and answers. To achieve good results, however, these models have been combined with additional features such as word overlap or BM25 scores. Without this combination, these models perform significantly worse than methods based on linguistic feature engineering. In this paper, we propose an attention based neural matching model for ranking short answer text. We adopt value-shared weighting scheme instead of position-shared weighting scheme for combining different matching signals and incorporate question term importance learning using question attention network. Using the popular benchmark TREC QA data, we show that the relatively simple aNMM model can significantly outperform other neural network models that have been used for the question answering task, and is competitive with models that are combined with additional features. When aNMM is combined with additional features, it outperforms all baselines.", "task": "question answering", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I propose a simple, attention-based neural matching model for question answering."}
{"documents": ["T-LESS"], "year": 2017, "keyphrase_query": "3d object detection, pose estimation image", "abstract": "We introduce a novel method for 3D object detection and pose estimation from color images only. We first use segmentation to detect the objects of interest in 2D even in presence of partial occlusions and cluttered background. By contrast with recent patch-based methods, we rely on a \"holistic\" approach: We apply to the detected objects a Convolutional Neural Network (CNN) trained to predict their 3D poses in the form of 2D projections of the corners of their 3D bounding boxes. This, however, is not sufficient for handling objects from the recent T-LESS dataset: These objects exhibit an axis of rotational symmetry, and the similarity of two images of such an object under two different poses makes training the CNN challenging. We solve this problem by restricting the range of poses used for training, and by introducing a classifier to identify the range of a pose at run-time before estimating it. We also use an optional additional step that refines the predicted poses. We improve the state-of-the-art on the LINEMOD dataset from 73.7% to 89.3% of correctly registered RGB frames. We are also the first to report results on the Occlusion dataset using color images only. We obtain 54% of frames passing the Pose 6D criterion on average on several sequences of the T-LESS dataset, compared to the 67% of the state-of-the-art on the same sequences which uses both color and depth. The full approach is also scalable, as a single network can be trained for multiple objects simultaneously.", "task": "3D object detection, pose estimation", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We introduce a novel method for 3D object detection and pose estimation from color images only."}
{"documents": ["Billion Word Benchmark", "WMT 2014"], "year": 2017, "keyphrase_query": "language modeling, machine translation text", "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.", "task": "language modeling, machine translation", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "", "query": "This work provides an improved technique for increasing model capacity with minor losses in computational efficiency, applied to language modeling and machine translation tasks"}
{"documents": ["IJB-A", "LFW"], "year": 2016, "keyphrase_query": "face verification image", "abstract": "Despite significant progress made over the past twenty five years, unconstrained face verification remains a challenging problem. This paper proposes an approach that couples a deep CNN-based approach with a low-dimensional discriminative embedding step, learned using triplet probability constraints to address the unconstrained face verification problem. Aside from yielding performance improvements, this embedding provides significant advantages in terms of memory and for post-processing operations like subject specific clustering. Experiments on the challenging IJB-A dataset show that the proposed algorithm performs close to the state of the art methods in verification and identification metrics, while requiring much less training data and training/test time. The superior performance of the proposed method on the CFP dataset shows that the representation learned by our deep CNN is robust to large pose variation. Furthermore, we demonstrate the robustness of deep features to challenges including age, pose, blur and clutter by performing simple clustering experiments on both IJB-A and LFW datasets.", "task": "face verification", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "I would like a dataset for face verification with subject variations in age, pose, blur and clutter."}
{"documents": ["PASCAL VOC"], "year": 2014, "keyphrase_query": "semantic image segmentation", "abstract": "Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called \"semantic image segmentation\"). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our \"DeepLab\" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.", "task": "semantic image segmentation", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "I propose a model for semantic image segmentation that combines neural networks with probabilistic graphical models"}
{"documents": ["COCO-QA", "Visual Question Answering"], "year": 2016, "keyphrase_query": "visual question answering image text", "abstract": "A number of recent works have proposed attention models for Visual Question Answering (VQA) that generate spatial maps highlighting image regions relevant to answering the question. In this paper, we argue that in addition to modeling \"where to look\" or visual attention, it is equally important to model \"what words to listen to\" or question attention. We present a novel co-attention model for VQA that jointly reasons about image and question attention. In addition, our model reasons about the question (and consequently the image via the co-attention mechanism) in a hierarchical fashion via a novel 1-dimensional convolution neural networks (CNN). Our model improves the state-of-the-art on the VQA dataset from 60.3% to 60.5%, and from 61.6% to 63.3% on the COCO-QA dataset. By using ResNet, the performance is further improved to 62.1% for VQA and 65.4% for COCO-QA.", "task": "Visual Question Answering", "domain": "", "modality": "image and text", "language": "", "training_style": "", "text_length": "", "query": "A novel co-attention model for VQA that jointly reasons about image and question attention"}
{"documents": ["STL-10"], "year": 2014, "keyphrase_query": "unsupervised feature learning image", "abstract": "We propose a meta-parameter free, off-the-shelf, simple and fast unsupervised feature learning algorithm, which exploits a new way of optimizing for sparsity. Experiments on STL-10 show that the method presents state-of-the-art performance and provides discriminative features that generalize well.", "task": "unsupervised feature learning", "domain": "", "modality": "Image", "language": "", "training_style": "Unsupervised", "text_length": "", "query": "We propose a new unsupervised feature learning algorithm that exploits a new way of optimizing for sparsity."}
{"documents": ["WMT 2015"], "year": 2015, "keyphrase_query": "machine translation text", "abstract": "Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only using parallel data for training. Target-side monolingual data plays an important role in boosting fluency for phrase-based statistical machine translation, and we investigate the use of monolingual data for NMT. In contrast to previous work, which combines NMT models with separately trained language models, we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model, and we explore strategies to train with monolingual data without changing the neural network architecture. By pairing monolingual training data with an automatic back-translation, we can treat it as additional parallel training data, and we obtain substantial improvements on the WMT 15 task English German (+2.8-3.7 BLEU), and for the low-resourced IWSLT 14 task Turkish->English (+2.1-3.4 BLEU), obtaining new state-of-the-art results. We also show that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task English->German.", "task": "machine translation", "domain": "", "modality": "Text", "language": "", "training_style": "Semi-supervised", "text_length": "", "query": "I investigate the use of monolingual data for neural machine translation."}
{"documents": ["LFW"], "year": 2015, "keyphrase_query": "face recognition image", "abstract": "Despite significant recent advances in the field of face recognition [10, 14, 15, 17], implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors.", "task": "face recognition", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "I need to build a scalable system for face recognition and face verification. "}
{"documents": ["MSRA-TD500", "Total-Text"], "year": 2018, "keyphrase_query": "scene text detection text,image", "abstract": "In this paper, we introduce a novel end-end framework for multi-oriented scene text detection from an instance-aware semantic segmentation perspective. We present Fused Text Segmentation Networks, which combine multi-level features during the feature extracting as text instance may rely on finer feature expression compared to general objects. It detects and segments the text instance jointly and simultaneously, leveraging merits from both semantic segmentation task and region proposal based object detection task. Not involving any extra pipelines, our approach surpasses the current state of the art on multi-oriented scene text detection benchmarks: ICDAR2015 Incidental Scene Text and MSRA-TD500 reaching Hmean 84.1% and 82.0% respectively. Morever, we report a baseline on total-text containing curved text which suggests effectiveness of the proposed approach.", "task": "scene text detection", "domain": "", "modality": "Text,Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I propose a novel method for scene text detection using instance-aware semantic segmentation."}
{"documents": ["AG News"], "year": 2019, "keyphrase_query": "text classification embedded systems mobile devices", "abstract": "Embedding artificial intelligence on constrained platforms has become a trend since the growth of embedded systems and mobile devices, experimented in recent years. Although constrained platforms do not have enough processing capabilities to train a sophisticated deep learning model, like convolutional neural networks (CNN), they are already capable of performing inference locally by using a previously trained embedded model. This approach enables numerous advantages such as privacy, response latency, and no real time network dependence. Still, the use of a local CNN model on constrained platforms is restricted by its storage size. Most of the research in CNNs has focused on increasing network depth to improve accuracy. In the text classification area, deep models were proposed with excellent performance but relying on large architectures with thousands of parameters, and consequently, high storage size. We propose to modify the structure of the Very Deep Convolutional Neural Networks (VDCNN) model to reduce its storage size while keeping the model performance. In this paper, we evaluate the impact of Temporal Depthwise Separable Convolutions and Global Average Pooling in the network parameters, storage size, dedicated hardware dependence, and accuracy. The proposed squeezed model (SVDCNN) is between 10x and 20x smaller than the original version, depending on the network depth, maintaining a maximum disk size of 6MB. Regarding accuracy, the network experiences a loss between 0.4% and 1.3% in the accuracy performance while obtains lower latency over non-dedicated hardware and higher inference time ratio compared to the baseline model.", "task": "text classification", "domain": "embedded systems and mobile devices", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "A very deep convolutional neural network to reduce its storage size while keeping the model performance for text classification task."}
{"documents": ["Indian Pines", "Pavia University"], "year": 2018, "keyphrase_query": "hyperspectral sensing image classification", "abstract": "Convolutional neural networks (CNNs) attained a good performance in hyperspectral sensing image (HSI) classification, but CNNs consider spectra as orderless vectors. Therefore, considering the spectra as sequences, recurrent neural networks (RNNs) have been applied in HSI classification, for RNNs is skilled at dealing with sequential data. However, for a long-sequence task, RNNs is difficult for training and not as effective as we expected. Besides, spatial contextual features are not considered in RNNs. In this study, we propose a Shorten Spatial-spectral RNN with Parallel-GRU (St-SS-pGRU) for HSI classification. A shorten RNN is more efficient and easier for training than band-by-band RNN. By combining converlusion layer, the St-SSpGRU model considers not only spectral but also spatial feature, which results in a better performance. An architecture named parallel-GRU is also proposed and applied in St-SS-pGRU. With this architecture, the model gets a better performance and is more robust.", "task": "hyperspectral sensing image classification", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to train a convolutional neural network to classify hyperspectral sensing images."}
{"documents": ["SCUT-CTW1500"], "year": 2018, "keyphrase_query": "text detection image", "abstract": "Traditional text detection methods mostly focus on quadrangle text. In this study we propose a novel method named sliding line point regression (SLPR) in order to detect arbitrary-shape text in natural scene. SLPR regresses multiple points on the edge of text line and then utilizes these points to sketch the outlines of the text. The proposed SLPR can be adapted to many object detection architectures such as Faster R-CNN and R-FCN. Specifically, we first generate the smallest rectangular box including the text with region proposal network (RPN), then isometrically regress the points on the edge of text by using the vertically and horizontally sliding lines. To make full use of information and reduce redundancy, we calculate x-coordinate or y-coordinate of target point by the rectangular box position, and just regress the remaining y-coordinate or x-coordinate. Accordingly we can not only reduce the parameters of system, but also restrain the points which will generate more regular polygon. Our approach achieved competitive results on traditional ICDAR2015 Incidental Scene Text benchmark and curve text detection dataset CTW1500.", "task": "text detection", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I aim to propose a novel method for arbitrary-shape text detection in natural scene."}
{"documents": ["WMT 2014"], "year": 2017, "keyphrase_query": "sequence modelling", "abstract": "There has been much recent work on training neural attention models at the sequence-level using either reinforcement learning-style methods or by optimizing the beam. In this paper, we survey a range of classical objective functions that have been widely used to train linear models for structured prediction and apply them to neural sequence to sequence models. Our experiments show that these losses can perform surprisingly well by slightly outperforming beam search optimization in a like for like setup. We also report new state of the art results on both IWSLT 2014 German-English translation as well as Gigaword abstractive summarization.", "task": "sequence modelling", "domain": "", "modality": "", "language": "", "training_style": "", "text_length": "", "query": "We want to apply classical objective functions to sequence-to-sequence tasks."}
